{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86284,"databundleVersionId":9813435,"sourceType":"competition"},{"sourceId":9658212,"sourceType":"datasetVersion","datasetId":5900333}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install jiwer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-18T10:17:19.972600Z","iopub.execute_input":"2024-10-18T10:17:19.972912Z","iopub.status.idle":"2024-10-18T10:17:34.369766Z","shell.execute_reply.started":"2024-10-18T10:17:19.972877Z","shell.execute_reply":"2024-10-18T10:17:34.368645Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nCollecting rapidfuzz<4,>=3 (from jiwer)\n  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading jiwer-3.0.4-py3-none-any.whl (21 kB)\nDownloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-3.0.4 rapidfuzz-3.10.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:17:34.371870Z","iopub.execute_input":"2024-10-18T10:17:34.372224Z","iopub.status.idle":"2024-10-18T10:17:57.276874Z","shell.execute_reply.started":"2024-10-18T10:17:34.372184Z","shell.execute_reply":"2024-10-18T10:17:57.275930Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting transformers\n  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed transformers-4.45.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import VisionEncoderDecoderModel, AutoProcessor\nfrom datasets import load_dataset\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm_notebook as tqdm\nfrom transformers import AdamW\nimport torch.nn.functional as F\ndevice='cuda' if torch.cuda.is_available() else 'cpu'\nfrom jiwer import wer","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:17:57.278276Z","iopub.execute_input":"2024-10-18T10:17:57.278602Z","iopub.status.idle":"2024-10-18T10:18:16.818743Z","shell.execute_reply.started":"2024-10-18T10:17:57.278567Z","shell.execute_reply":"2024-10-18T10:18:16.817969Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load the model and processor\nmodel = VisionEncoderDecoderModel.from_pretrained(\"qantev/trocr-large-spanish\")\nprocessor = AutoProcessor.from_pretrained(\"qantev/trocr-large-spanish\")\ntokenizer = TrOCRProcessor.from_pretrained('qantev/trocr-large-spanish')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:18:16.820743Z","iopub.execute_input":"2024-10-18T10:18:16.821366Z","iopub.status.idle":"2024-10-18T10:18:47.735925Z","shell.execute_reply.started":"2024-10-18T10:18:16.821330Z","shell.execute_reply":"2024-10-18T10:18:47.735109Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48a1df55f0a943edbb809dc87cacb46b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e72c45f691f4484e8e082fd8ddfc3296"}},"metadata":{}},{"name":"stderr","text":"VisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/420 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"784762c890d444dc8e3bd069d1da773a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/364 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0011ff65daf24c6a8113a6bc92b20249"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb3c467a3b8e4dab98c7644fbed8d067"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c20366d0bc8741b28381f13ba5db3cf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86e1f47265624a10ae31298daaad55d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbe061dacf6448689808bafe21bbafa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8221dd3412e4927a7c7dadb73d49c70"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"/kaggle/input/large-trocr-8-epochs-post-process/model_weights_large_grayscale_contrast (1).pth\"))","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:18:47.737052Z","iopub.execute_input":"2024-10-18T10:18:47.737358Z","iopub.status.idle":"2024-10-18T10:19:05.890089Z","shell.execute_reply.started":"2024-10-18T10:18:47.737326Z","shell.execute_reply":"2024-10-18T10:19:05.889192Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3069459000.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/input/large-trocr-8-epochs-post-process/model_weights_large_grayscale_contrast (1).pth\"))\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/ai-of-god-3/Public_data/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:19:05.891454Z","iopub.execute_input":"2024-10-18T10:19:05.891771Z","iopub.status.idle":"2024-10-18T10:19:05.906790Z","shell.execute_reply.started":"2024-10-18T10:19:05.891739Z","shell.execute_reply":"2024-10-18T10:19:05.905898Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"test_path = '/kaggle/input/ai-of-god-3/Public_data/test_images'\n\n# Function to convert unique ID into correct image path\ndef construct_image_path(unique_id):\n    parts = unique_id.split('_')\n    page_number = parts[1]\n    l_number = parts[3] \n    \n    image_path = os.path.join(test_path, f'Page_{page_number}', f'L_{l_number}.png')\n    return image_path\n\ntest_df['image_path'] = test_df['unique Id'].apply(construct_image_path)\n\n# Check the resulting dataframe\nprint(test_df[[ 'image_path']].head())","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:19:05.907942Z","iopub.execute_input":"2024-10-18T10:19:05.908272Z","iopub.status.idle":"2024-10-18T10:19:05.927833Z","shell.execute_reply.started":"2024-10-18T10:19:05.908240Z","shell.execute_reply":"2024-10-18T10:19:05.926871Z"},"trusted":true},"outputs":[{"name":"stdout","text":"                                          image_path\n0  /kaggle/input/ai-of-god-3/Public_data/test_ima...\n1  /kaggle/input/ai-of-god-3/Public_data/test_ima...\n2  /kaggle/input/ai-of-god-3/Public_data/test_ima...\n3  /kaggle/input/ai-of-god-3/Public_data/test_ima...\n4  /kaggle/input/ai-of-god-3/Public_data/test_ima...\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from torchvision import transforms","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:19:05.929052Z","iopub.execute_input":"2024-10-18T10:19:05.929413Z","iopub.status.idle":"2024-10-18T10:19:05.933989Z","shell.execute_reply.started":"2024-10-18T10:19:05.929376Z","shell.execute_reply":"2024-10-18T10:19:05.933061Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:19:05.935225Z","iopub.execute_input":"2024-10-18T10:19:05.935549Z","iopub.status.idle":"2024-10-18T10:19:06.621866Z","shell.execute_reply.started":"2024-10-18T10:19:05.935517Z","shell.execute_reply":"2024-10-18T10:19:06.620894Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"VisionEncoderDecoderModel(\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=False)\n              (key): Linear(in_features=1024, out_features=1024, bias=False)\n              (value): Linear(in_features=1024, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n        (embed_positions): TrOCRSinusoidalPositionalEmbedding()\n        (layers): ModuleList(\n          (0-11): 12 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n  )\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# predictions = []\n\n# model.eval()\n\n# with torch.no_grad():\n#     for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Inference\"):\n#         image_path = row['image_path']\n        \n#         # Open and preprocess the image (convert to grayscale, normalize, etc.)\n#         image = Image.open(image_path).convert(\"RGB\")\n        \n#         # Apply the transformations (grayscale, normalize, and ToTensor)\n#         transform = transforms.Compose([\n#             transforms.Grayscale(num_output_channels=3),\n#             transforms.ToTensor()\n#         ])\n        \n#         image = transform(image)\n\n#         image_g = image.to(device)\n\n#         mask = image_g < 0.6875\n#         image_g[mask] = 0.0\n#         image_g[~mask] = 1.0\n        \n#         # Process the image using the processor\n#         pixel_values = processor(image_g, return_tensors=\"pt\", do_rescale=False).pixel_values.to(device)\n\n#         # Generate output\n#         outputs = model.generate(pixel_values)\n\n#         # Decode the predicted string\n#         predicted_string = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n#         # Append the results\n#         predictions.append({\n#             'unique Id': row['unique Id'],  # Ensure 'unique Id' is correctly named in the dataframe\n#             'prediction': predicted_string\n#         })\n\n# # Create DataFrame for predictions\n# predictions_df = pd.DataFrame(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:19:06.625343Z","iopub.execute_input":"2024-10-18T10:19:06.625669Z","iopub.status.idle":"2024-10-18T10:19:06.631236Z","shell.execute_reply.started":"2024-10-18T10:19:06.625630Z","shell.execute_reply":"2024-10-18T10:19:06.630301Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# kernel = np.ones((2,2),np.uint8)\n# dilated_img = cv2.dilate(img, kernel, iterations=1)\n# eroded_img = cv2.erode(dilated_img, kernel, iterations=1)\n\n# coords = np.column_stack(np.where(img > 0))\n# angle = cv2.minAreaRect(coords)[-1]\n# if angle < -45:\n#     angle = -(90 + angle)\n# else:\n#     angle = -angle\n# (h, w) = img.shape[:2]\n# center = (w // 2, h // 2)\n# M = cv2.getRotationMatrix2D(center, angle, 1.0)\n# deskewed_img = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n\n# sharpen_kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n# sharpened_img = cv2.filter2D(img, -1, sharpen_kernel)\n\n# M = cv2.getRotationMatrix2D(center, 15, 1.0)  # rotate by 15 degrees\n# rotated_img = cv2.warpAffine(img, M, (w, h))\n\n\n# contrast_img = cv2.convertScaleAbs(img, alpha=1.5, beta=0)  # alpha for contrast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T10:19:06.632436Z","iopub.execute_input":"2024-10-18T10:19:06.632813Z","iopub.status.idle":"2024-10-18T10:19:06.645472Z","shell.execute_reply.started":"2024-10-18T10:19:06.632768Z","shell.execute_reply":"2024-10-18T10:19:06.644600Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# # Example of applying warp affine transformation to \"fix\" irregular handwritten text\n# M = np.float32([[1, 0, 0], [0.3, 1, 0]])\n# warped_img = cv2.warpAffine(img, M, (w, h))\n\n# # Example of baseline detection and alignment using Hough Transform\n# lines = cv2.HoughLinesP(img, 1, np.pi / 180, 100)\n# for line in lines:\n#     x1, y1, x2, y2 = line[0]\n#     cv2.line(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n# edges = cv2.Canny(img, 100, 200)\n# enhanced_img = cv2.addWeighted(img, 1.5, edges, -0.5, 0)\n\n# from PIL import Image, ImageDraw, ImageFont\n# font = ImageFont.truetype('Arial.ttf', size=45)\n# image = Image.new('RGB', (512, 256), color = (255, 255, 255))\n# draw = ImageDraw.Draw(image)\n# draw.text((50, 100), \"Printed text\", font=font, fill=(0, 0, 0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T10:19:06.646485Z","iopub.execute_input":"2024-10-18T10:19:06.646775Z","iopub.status.idle":"2024-10-18T10:19:06.656804Z","shell.execute_reply.started":"2024-10-18T10:19:06.646737Z","shell.execute_reply":"2024-10-18T10:19:06.655899Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"predictions = []\n\n# Set the model to evaluation mode\nmodel.eval()\n\nwith torch.no_grad():  # No need to calculate gradients during inference\n    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Inference\"):\n        image_path = row['image_path']\n        \n        # Open the image as grayscale\n        image = Image.open(image_path).convert(\"L\")  # 'L' mode for grayscale\n        \n        # Transform the image to tensor format\n        transform = transforms.ToTensor()\n        image_tensor = transform(image)  # Shape: [1, H, W] for grayscale\n        \n        # Stack the grayscale image to 3 channels to match model input\n        image_stacked = torch.cat([image_tensor] * 3, dim=0)  # Shape: [3, H, W]\n\n        # Apply threshold transformation\n        mask = image_stacked < 0.6875\n        image_stacked[mask] = 0.0\n        image_stacked[~mask] = 1.0\n\n        # Reshape the image for batch processing\n        image_stacked = image_stacked.unsqueeze(0)  # Add batch dimension, Shape: [1, 3, H, W]\n\n        # Process the image using the processor without rescaling (as per your requirement)\n        pixel_values = processor(image_stacked, return_tensors=\"pt\", do_rescale=False).pixel_values.to(device)\n        \n        # Generate the output using the model\n        outputs = model.generate(pixel_values)\n\n        # Decode the generated output to get the predicted string\n        predicted_string = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Append the results to predictions list\n        predictions.append({\n            'unique Id': row['unique Id'],  # Ensure 'unique Id' matches your column name\n            'prediction': predicted_string\n        })\n\n# Create a DataFrame for storing predictions\npredictions_df = pd.DataFrame(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T10:19:06.657964Z","iopub.execute_input":"2024-10-18T10:19:06.658277Z","iopub.status.idle":"2024-10-18T10:20:00.966125Z","shell.execute_reply.started":"2024-10-18T10:19:06.658245Z","shell.execute_reply":"2024-10-18T10:20:00.965186Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Inference:   0%|          | 0/168 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0088e948e374f458e2dc083e235a9fd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1338: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import re\n\ndef preprocess_transcription(text):\n    \"\"\"\n    Preprocess the transcriptions based on Old Spanish characteristics.\n    \"\"\"\n    # Handle interchangeable letters\n    text = re.sub(r'(?<!\\S)[uU](?!\\S)', 'v', text)  # u -> v\n    text = re.sub(r'(?<!\\S)[vV](?!\\S)', 'u', text)  # v -> u\n    text = re.sub(r'(?<!\\S)[fF](?!\\S)', 's', text)  # f -> s\n    text = re.sub(r'(?<!\\S)[sS](?!\\S)', 'f', text)  # s -> f\n    \n    # Remove accents\n    text = re.sub(r'[√°√†√§√¢]', 'a', text)\n    text = re.sub(r'[√©√®√´√™]', 'e', text)\n    text = re.sub(r'[√≠√¨√Ø√Æ]', 'i', text)\n    text = re.sub(r'[√≥√≤√∂√¥]', 'o', text)\n    text = re.sub(r'[√∫√π√º√ª]', 'u', text)\n\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T10:20:00.967550Z","iopub.execute_input":"2024-10-18T10:20:00.968026Z","iopub.status.idle":"2024-10-18T10:20:00.976532Z","shell.execute_reply.started":"2024-10-18T10:20:00.967981Z","shell.execute_reply":"2024-10-18T10:20:00.975600Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"predictions_df['postprocessed_transcription'] = predictions_df['prediction'].apply(preprocess_transcription)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T10:20:00.977791Z","iopub.execute_input":"2024-10-18T10:20:00.978054Z","iopub.status.idle":"2024-10-18T10:20:00.999494Z","shell.execute_reply.started":"2024-10-18T10:20:00.978025Z","shell.execute_reply":"2024-10-18T10:20:00.998717Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def add_quotes(value):\n    # Check if the value is not already enclosed in double quotes\n    if not (isinstance(value, str) and value.startswith('\"') and value.endswith('\"')):\n        return f'\"{value}\"'  # Add double quotes\n    return value ","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:20:01.000499Z","iopub.execute_input":"2024-10-18T10:20:01.000804Z","iopub.status.idle":"2024-10-18T10:20:01.010866Z","shell.execute_reply.started":"2024-10-18T10:20:01.000773Z","shell.execute_reply":"2024-10-18T10:20:01.009952Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def remove_quotes(value):\n    # Check if the value is a string\n    if isinstance(value, str):\n        # Remove leading and trailing double quotes\n        return value.strip('\"')\n    return value","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:20:01.011934Z","iopub.execute_input":"2024-10-18T10:20:01.012317Z","iopub.status.idle":"2024-10-18T10:20:01.021010Z","shell.execute_reply.started":"2024-10-18T10:20:01.012285Z","shell.execute_reply":"2024-10-18T10:20:01.020091Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"predictions_df['prediction']=predictions_df['prediction'].apply(remove_quotes)\nprint(predictions_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:20:01.022009Z","iopub.execute_input":"2024-10-18T10:20:01.022320Z","iopub.status.idle":"2024-10-18T10:20:01.035674Z","shell.execute_reply.started":"2024-10-18T10:20:01.022264Z","shell.execute_reply":"2024-10-18T10:20:01.034806Z"},"trusted":true},"outputs":[{"name":"stdout","text":"    unique Id                              prediction  \\\n0     P_1_L_1  crio se√±or por sus rectifsimos y o cul   \n1     P_1_L_2       tos juyrios de darme tiempo, para   \n2     P_1_L_3        que en ruestra educacion lo mani   \n3     P_1_L_4    tellalle, pues me hallo tantos meses   \n4     P_1_L_5       ha tendida en esta cama, √° vna en   \n..        ...                                     ...   \n163  P_7_L_20         Ciceron del Noble, y creed, que   \n164  P_7_L_21     son talentos de que le aueys de dar   \n165  P_7_L_22        cuenta muy estrecha, y carga con   \n166  P_7_L_23       que os ha obligado √° ser defenfor   \n167  P_7_L_24     de su F≈´ y Religion, accerrimo per-   \n\n                postprocessed_transcription  \n0    crio se√±or por sus rectifsimos y o cul  \n1         tos juyrios de darme tiempo, para  \n2          que en ruestra educacion lo mani  \n3      tellalle, pues me hallo tantos meses  \n4         ha tendida en esta cama, a vna en  \n..                                      ...  \n163         Ciceron del Noble, y creed, que  \n164     son talentos de que le aueys de dar  \n165        cuenta muy estrecha, y carga con  \n166       que os ha obligado a ser defenfor  \n167     de su F≈´ y Religion, accerrimo per-  \n\n[168 rows x 3 columns]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"predictions_df = predictions_df.drop('prediction', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T10:20:01.036644Z","iopub.execute_input":"2024-10-18T10:20:01.036963Z","iopub.status.idle":"2024-10-18T10:20:01.044855Z","shell.execute_reply.started":"2024-10-18T10:20:01.036932Z","shell.execute_reply":"2024-10-18T10:20:01.044061Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"predictions_df = predictions_df.rename(columns={'postprocessed_transcription': 'prediction'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T10:20:01.045861Z","iopub.execute_input":"2024-10-18T10:20:01.046125Z","iopub.status.idle":"2024-10-18T10:20:01.057735Z","shell.execute_reply.started":"2024-10-18T10:20:01.046096Z","shell.execute_reply":"2024-10-18T10:20:01.056962Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"predictions_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:20:01.058860Z","iopub.execute_input":"2024-10-18T10:20:01.059174Z","iopub.status.idle":"2024-10-18T10:20:01.071214Z","shell.execute_reply.started":"2024-10-18T10:20:01.059118Z","shell.execute_reply":"2024-10-18T10:20:01.070209Z"},"trusted":true},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"  unique Id                              prediction\n0   P_1_L_1  crio se√±or por sus rectifsimos y o cul\n1   P_1_L_2       tos juyrios de darme tiempo, para\n2   P_1_L_3        que en ruestra educacion lo mani\n3   P_1_L_4    tellalle, pues me hallo tantos meses\n4   P_1_L_5       ha tendida en esta cama, a vna en","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique Id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P_1_L_1</td>\n      <td>crio se√±or por sus rectifsimos y o cul</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P_1_L_2</td>\n      <td>tos juyrios de darme tiempo, para</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>P_1_L_3</td>\n      <td>que en ruestra educacion lo mani</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>P_1_L_4</td>\n      <td>tellalle, pues me hallo tantos meses</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P_1_L_5</td>\n      <td>ha tendida en esta cama, a vna en</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"predictions_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T10:20:01.072400Z","iopub.execute_input":"2024-10-18T10:20:01.073612Z","iopub.status.idle":"2024-10-18T10:20:01.082245Z","shell.execute_reply.started":"2024-10-18T10:20:01.073551Z","shell.execute_reply":"2024-10-18T10:20:01.081363Z"},"trusted":true},"outputs":[],"execution_count":22}]}